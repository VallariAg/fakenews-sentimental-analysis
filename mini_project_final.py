# -*- coding: utf-8 -*-
"""Mini-Project Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xvvEeWYSQvmN1fh2J1EDDIFA-qb170DV

# Fake Tweet Detection
"""

# !df -h

# !cat /proc/cpuinfo

# !cat /proc/meminfo

import numpy as np
import pandas as pd
import re
import tweepy
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
# import pickle
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from collections import Counter
import joblib
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
import scipy.stats as chi2_contingency
from xgboost import XGBClassifier
import matplotlib.pyplot as plt

nltk.download('stopwords')

# Dataset_Fake News Detection
df=pd.read_csv('english1.csv')
df.head()

print(df.groupby('label').count())

X = df['tweet']
Y = df['label']
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Y = le.fit_transform(Y)
print(stopwords.words('english'))

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

df['tweet'] = df['tweet'].apply(stemming)

X = df['tweet'].values

fakenews_vectorizer = TfidfVectorizer()
fakenews_vectorizer.fit(X)

X = fakenews_vectorizer.transform(X)

# model
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)
# ML Models
ns_probs = [0 for _ in range(len(Y_test))]
svm_cf = svm.SVC(kernel='rbf', random_state = 0, probability=True)
lr_cf = LogisticRegression(solver='sag', random_state=0, C=5, max_iter=1000)
from sklearn.tree import DecisionTreeClassifier
d_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
from sklearn.naive_bayes import GaussianNB
mnb_cf = MultinomialNB()
rfc_model = RandomForestClassifier()
knn_cf = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
avg_cf = VotingClassifier(estimators=[('svm', svm_cf), ('lr', lr_cf),  ('rf', rfc_model) ], voting='soft', weights=[2,1,1])
avg_cf.fit(X_train, Y_train)

X_train_prediction = avg_cf.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
f1_trainf=f1_score(X_train_prediction, Y_train)
precision_train = precision_score(X_train_prediction, Y_train)
print(classification_report(X_train_prediction, Y_train))
print('Accuracy score of the training data : ', training_data_accuracy)
print('F-1 Score : ', f1_trainf)
print('Precision : ', precision_train)
recall_train = recall_score(X_train_prediction, Y_train)
print('Recall: %f' % recall_train)

X_test_prediction = avg_cf.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)
f1_testf = f1_score(X_test_prediction, Y_test)
precision_test = precision_score(X_test_prediction, Y_test)
print(classification_report(X_test_prediction, Y_test))
print('Accuracy score of the test data : ', test_data_accuracy)
print('F-1 Score : ', f1_testf)
print('Precision : ', precision_test)
recall_test = recall_score(X_test_prediction, Y_test)
print('Recall: %f' % recall_test)

# Confusion Matrix
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt

plot_confusion_matrix(avg_cf, X_test, Y_test, cmap=plt.cm.Blues)
plt.show()

plt.figure(0).clf()

#fit logistic regression model and plot ROC curve
lr_cf.fit(X_train, Y_train)
y_pred = lr_cf.predict(X_test)
fpr, tpr, _ = metrics.roc_curve(Y_test, y_pred)
auc = round(metrics.roc_auc_score(Y_test, y_pred), 4)
plt.plot(fpr,tpr,label="Logistic Regression, AUC="+str(auc))

#fit gradient boosted model and plot ROC curve
y_pred = avg_cf.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(X_test_prediction, Y_test)
auc = round(metrics.roc_auc_score(X_test_prediction, Y_test), 4)
plt.plot(fpr,tpr,label="Gradient Boosting, AUC="+str(auc))
plt.legend()


"""# Sentiment Anaysis"""

from sklearn.base import BaseEstimator, TransformerMixin

class InputTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, vectorizer):
        self.vectorizer = vectorizer
        print("initalized InputTransformer")

    def fit(self, A, B=None):
        print('fit')
        return self

    def transform(self, A, B=None):
        A_ = A.copy()
        print('trasform')
        transformedX = self.vectorizer.transform(A_)
        return transformedX

# import database
import pandas as pd
dataset = pd.read_csv('senti.csv')
dataset.head()

A = dataset["Tweets"]
B = dataset["Sentiment_Label"]

print(dataset.groupby('Sentiment_Label').count())

from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
# import pickle
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from collections import Counter
import joblib
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB


# Splitting the data into train
A_train, A_test, B_train, B_test = train_test_split(A, B, test_size=0.2)

# ML Models
s_svm_cf = svm.SVC(kernel='rbf', random_state = 0)
s_lr_cf = LogisticRegression(solver='sag', random_state=0, C=5, max_iter=1000)
s_knn_cf = KNeighborsClassifier(n_neighbors=5)
s_dtr_model = DecisionTreeClassifier()
s_rfc_model = RandomForestClassifier()
s_mnb_cf = MultinomialNB()
x_cf = XGBClassifier()
sentiment_avg_cf = VotingClassifier(estimators=[('svm', s_svm_cf), ('rf', s_rfc_model), ('lr', s_lr_cf)], voting='hard')


vectorizer = TfidfVectorizer(use_idf = True, max_df = 200)
vectorizer.fit_transform(A)


pipeline = Pipeline([('preprocessing', InputTransformer(vectorizer)), ('average_model', sentiment_avg_cf)])

pipeline.fit(A_train, B_train)
print("pipeline fit-ed")


B_pred = pipeline.predict(A_test)
accuracy = metrics.accuracy_score(B_test, B_pred)
print(classification_report(B_test, B_pred))
print("\nAccuracy - ", accuracy)
f1_tests = f1_score(B_test, B_pred, average="weighted")
precision = precision_score(B_test, B_pred, average="weighted")
print('F-1 Score : ', f1_tests)
print('Precision : ', precision)
recall = recall_score(B_test, B_pred, average="weighted")
print('Recall: %f' % recall)

# confusion matrix
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt

plot_confusion_matrix(pipeline, A_test, B_test, cmap=plt.cm.Blues)
plt.show()

"""# Final flow"""

# Twitter API - fetch that from Twitter Developer accounts
import tweepy
consumer_key=""
consumer_secret=""
access_token=""
access_token_secret=""

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit= True)

def get_replies(tweet_id, tweet_username):
  replies = tweepy.Cursor(api.search, q='to:{}'.format(tweet_username),
                                since_id=tweet_id, tweet_mode='extended').items()
  print(replies)
  return replies

import pandas as pd
import csv

search_words = "covid -filter:retweets"
dataset_size = 1000

tweets = tweepy.Cursor(api.search,
              q=search_words,
              lang="en").items(dataset_size)

raw_dataset = []

for tweet in tweets:
    raw_dataset += [{"tweet_text": tweet.text, "username": tweet.user.screen_name, "tweet_id": tweet.id }]

raw_dataset = pd.DataFrame(raw_dataset)
print(raw_dataset)

# preprcosessing
raw_dataset['tweet_text'] = raw_dataset['tweet_text'].apply(stemming)
raw_text = raw_dataset['tweet_text'].values

raw_text_t = fakenews_vectorizer.transform(raw_text)

# fake news model
fake_labels__ = avg_cf.predict(raw_text_t)

raw_dataset["fake_news_label"] = fake_labels__

fake_tweet = raw_dataset.loc[raw_dataset['fake_news_label'] == 1]
fake_tweet

len(raw_data_fake_labelled)

raw_dataset = raw_dataset.reset_index()
import csv

f = open('./replies_real.csv', 'w')
writer = csv.writer(f)

replies_dataset = []
for index, row in fake_tweet.iterrows():
  try:
    replies = tweepy.Cursor(api.search, q='to:{}'.format(row["username"]),
                                since_id=row["tweet_id"], tweet_mode='extended').items()
    for reply in replies:
      tweet_text = reply.full_text
      tweet_text = tweet_text.replace(',', ' ').replace("\n", " ")
      writer.writerow([tweet_text])
      replies_dataset += [reply.full_text]

  except:
    print("error")
print(replies_dataset)

f.close()

sentiment_results = pipeline.predict(replies_dataset)
sentiment_results

replies_labelled_dataset = pd.DataFrame(replies_dataset)
replies_labelled_dataset["label"] = sentiment_results

from collections import Counter

fakenews_counter = Counter(fake_labels__)
sentiment_counter = Counter(sentiment_results)
print(sentiment_counter)
print(fakenews_counter)

import matplotlib.pyplot as plt
import numpy as np

y = np.array(list(sentiment_counter.values()))
mylabels = ["negative", "neutral", "positive"]

plt.bar(mylabels, y)

plt.show()

import matplotlib.pyplot as plt


plt.pie(list(fakenews_counter.values()), labels = ["fake", "real"])
plt.show()

replies_real=pd.read_csv('replies_real.csv')
sentiment_results = pipeline.predict(replies_real[replies_real.columns[0]])
replies_real["label"] = sentiment_results

print(replies_real)
replies_real.to_csv("replies_real_labelled.csv", sep=',')

import pandas as pd
dataset = pd.read_csv('senti.csv')
dataset.head()

from collections import Counter


negative = list(dataset.loc[dataset['Sentiment_Label'] == 'negative']["Tweets"])
neutral = list(dataset.loc[dataset['Sentiment_Label'] == 'neutral']["Tweets"])
positive = list(dataset.loc[dataset['Sentiment_Label'] == 'positive']["Tweets"])

negative = " ".join(negative)
positive = " ".join(positive)
neutral = " ".join(neutral)

negative = negative.split()
positive = positive.split()
neutral = neutral.split()

negative = Counter(negative)
positive = Counter(positive)
neutral = Counter(neutral)

print(neutral.most_common(20))
print(positive.most_common(20))
print(negative.most_common(20))

neg = set(dict(negative.most_common(200)).keys())
pos = set(dict(positive.most_common(200)).keys())
neu = set(dict(neutral.most_common(200)).keys())

neg_s = " ".join(neg - pos - neu)
pos_s = " ".join(pos - neu - neg - set(["lol", "skull", "gabbymar", "resolut", "incr"]))
neu_s = " ".join(neu - pos - neg)

print(neg - pos - neu)
print(pos - neu - neg)
print(neu - pos - neg)

fake_count = df['predictions'].value_counts()[0]
real_count = df['predictions'].value_counts()[1]
df = pd.DataFrame({'label':['0', '1'], 'val':[fake_count, real_count]})

ax = df.plot.bar(x='lab', y='val', rot=0)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(neg_s)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(neu_s)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(pos_s)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

import pandas as pd
dataset_fake = pd.read_csv('english1.csv')
dataset_fake.head()

from collections import Counter

dataset_fake["tweet"] = dataset_fake['tweet'].apply(stemming)

real = list(dataset_fake.loc[dataset_fake['label'] == 'real']["tweet"])
fake = list(dataset_fake.loc[dataset_fake['label'] == 'fake']["tweet"])


fake = " ".join(fake)
real = " ".join(real)

fake = fake.split()
real = real.split()

fake = Counter(fake)
real = Counter(real)

fake_c = set(dict(fake.most_common(200)).keys())
real_c = set(dict(real.most_common(200)).keys())

real_s = " ".join(real_c - fake_c)
fake_s = " ".join(fake_c - real_c)

print(real_s)
print(fake_s)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(real_s)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
word_cloud = WordCloud(collocations = False, background_color = 'white').generate(fake_s)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# MENTAL HEALTH RESULT
import pandas as pd
gdf = pd.DataFrame({'Mental Health Predictions':['0', '1', '2'], 'Prediction Count':[2400, 1800, 6200]})

gdf.groupby(['Mental Health Predictions']).sum().plot(kind='pie', y='Prediction Count', autopct='%1.0f%%')